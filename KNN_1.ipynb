{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is the KNN Algorithm?  \n",
        "The **K-Nearest Neighbors (KNN)** algorithm is a simple, non-parametric, lazy learning algorithm used for classification and regression tasks. It works by:  \n",
        "1. Calculating the distance between a new data point and all other points in the training dataset.  \n",
        "2. Selecting the **K** nearest data points (neighbors).  \n",
        "3. For classification: Assigning the majority class among the neighbors.  \n",
        "4. For regression: Averaging the target values of the neighbors.  \n",
        "\n",
        "---\n",
        "\n",
        "### Q2. How do you choose the value of K in KNN?  \n",
        "Choosing the value of **K** depends on:  \n",
        "1. **Odd values for binary classification**: To avoid ties.  \n",
        "2. **Cross-validation**: Testing different values of K and selecting the one with the best performance.  \n",
        "3. **General guideline**: A small K may lead to overfitting (sensitive to noise), while a large K may cause underfitting (too generalized).  \n",
        "\n",
        "---\n",
        "\n",
        "### Q3. What is the difference between KNN Classifier and KNN Regressor?  \n",
        "| Aspect               | KNN Classifier                  | KNN Regressor                    |  \n",
        "|----------------------|---------------------------------|----------------------------------|  \n",
        "| **Output**           | Class label (discrete)         | Continuous value (numeric)       |  \n",
        "| **Decision Rule**    | Majority voting among neighbors | Average of neighbors' target values |  \n",
        "| **Use Case**         | Classification problems (e.g., spam detection) | Regression problems (e.g., predicting house prices) |  \n",
        "\n",
        "---\n",
        "\n",
        "### Q4. How do you measure the performance of KNN?  \n",
        "1. **For Classification**:  \n",
        "   - Accuracy, Precision, Recall, F1-score, ROC-AUC.  \n",
        "   - Confusion matrix for detailed insights.  \n",
        "2. **For Regression**:  \n",
        "   - Mean Squared Error (MSE), Mean Absolute Error (MAE), R² (coefficient of determination).  \n",
        "3. Using cross-validation to evaluate the stability of the model.  \n",
        "\n",
        "---\n",
        "\n",
        "### Q5. What is the Curse of Dimensionality in KNN?  \n",
        "The **curse of dimensionality** refers to the challenges that arise when the number of features (dimensions) increases:  \n",
        "1. Distances between points become less meaningful.  \n",
        "2. Sparsity in high-dimensional space reduces the algorithm's effectiveness.  \n",
        "3. Computational cost increases due to more distance calculations.  \n",
        "\n",
        "**Solution**: Use dimensionality reduction techniques like PCA or feature selection.  \n",
        "\n",
        "---\n",
        "\n",
        "### Q6. How do you handle missing values in KNN?  \n",
        "1. **Imputation**:  \n",
        "   - Replace missing values with the mean, median, or mode.  \n",
        "   - Use **KNN Imputation**: Replace missing values by finding K nearest neighbors and using their average (regression) or majority class (classification).  \n",
        "2. **Remove instances**: If there are too many missing values and data loss is acceptable.  \n",
        "\n",
        "---\n",
        "\n",
        "### Q7. Compare and Contrast the Performance of the KNN Classifier and Regressor  \n",
        "| Aspect                       | KNN Classifier                         | KNN Regressor                         |  \n",
        "|------------------------------|----------------------------------------|---------------------------------------|  \n",
        "| **Better for**               | Discrete, categorical outputs (e.g., spam detection) | Continuous outputs (e.g., predicting prices) |  \n",
        "| **Sensitive to noise**       | Less sensitive due to majority voting | More sensitive due to averaging |  \n",
        "| **Metric choice**            | Accuracy, Precision, Recall           | MSE, MAE, R²                          |  \n",
        "| **Performance factor**       | Depends on class distribution          | Depends on value distribution         |  \n",
        "\n",
        "---\n",
        "\n",
        "### Q8. Strengths and Weaknesses of the KNN Algorithm  \n",
        "**Strengths**:  \n",
        "1. Simple and intuitive.  \n",
        "2. Non-parametric (no assumptions about data distribution).  \n",
        "3. Effective for small datasets with well-separated classes.  \n",
        "\n",
        "**Weaknesses**:  \n",
        "1. Computationally expensive for large datasets.  \n",
        "2. Sensitive to irrelevant features and noise.  \n",
        "3. Poor performance in high-dimensional spaces.  \n",
        "\n",
        "**Solutions**:  \n",
        "1. Use efficient distance computation (e.g., KD-Tree, Ball Tree).  \n",
        "2. Apply feature scaling and selection.  \n",
        "3. Use dimensionality reduction techniques.  \n",
        "\n",
        "---\n",
        "\n",
        "### Q9. What is the Difference Between Euclidean Distance and Manhattan Distance in KNN?  \n",
        "| Feature                  | Euclidean Distance                    | Manhattan Distance                   |  \n",
        "|--------------------------|---------------------------------------|--------------------------------------|  \n",
        "| **Formula**              | \\( \\sqrt{\\sum (x_i - y_i)^2} \\)      | \\( \\sum |x_i - y_i| \\)               |  \n",
        "| **Interpretation**       | Straight-line distance (as-the-crow-flies) | Sum of absolute differences (grid-based) |  \n",
        "| **Sensitive to scale**   | More sensitive to large differences. | Less sensitive compared to Euclidean. |  \n",
        "| **Use Case**             | Continuous data.                     | Data with grid-like features.        |  \n",
        "\n",
        "---\n",
        "\n",
        "### Q10. What is the Role of Feature Scaling in KNN?  \n",
        "**Feature scaling** ensures that all features contribute equally to the distance computation in KNN.  \n",
        "1. Without scaling: Features with larger ranges dominate the distance calculation.  \n",
        "2. With scaling (e.g., StandardScaler, MinMaxScaler):  \n",
        "   - All features are normalized to the same scale.  \n",
        "   - Improves performance and avoids bias towards features with larger magnitudes.  \n",
        "\n",
        "Would you like examples or visualizations for any of these concepts?"
      ],
      "metadata": {
        "id": "K3VUKMbTtUb5"
      }
    }
  ]
}